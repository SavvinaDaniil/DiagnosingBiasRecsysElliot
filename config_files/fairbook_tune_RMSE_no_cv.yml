experiment:
  print_results_as_triplets: True # method, metric, value
  dataset: fairbook # data set choice
  data_config:
    strategy: dataset # whether the splitting is done by elliot or we're already drawing from train-test
    dataset_path: ../data/fairbook/fairbook_events.tsv

  
  top_k: 10 # length of recommended list
  evaluation:
    # cutoffs: [10] # eg nDCG@5 and @10
    # relevance_threshold: 0 # minimum value of rating to consider a test transcation relevant for the evaluation process, 0 is the default
    # paired_ttest: True # whether there will be significance tests
    # wilcoxon_test: True
    simple_metrics: [RMSE] # skip RMSE for now because I think it results in weird results
  # gpu: -1
  splitting:
    test_splitting:
        strategy: random_subsampling
        test_ratio: 0.2
  models:
    DMF:
      meta:
        
        hyper_opt_alg: tpe
        hyper_max_evals: 5
        validation_rate: 1
        verbose: True
        # save_weights: True
        validation_metric: RMSE
        save_recs: True
        
      user_mlp: (64,32)
      item_mlp: (64,32)
      epochs: 10 #ok
      batch_size: 256 #ok
      lr: [0.1, 0.01, 0.001, 0.0001] #ok
      reg: [1, 0.1, 0.01, 0.001] #ok 
      similarity: cosine
      n_fold: 1