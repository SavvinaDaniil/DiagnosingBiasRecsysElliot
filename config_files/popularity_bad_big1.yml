experiment:
  print_results_as_triplets: True # method, metric, value
  dataset: popularity_bad_big1 # data set choice
  data_config:
    strategy: fixed # whether the splitting is done by elliot or we're already drawing from train-test
    # dataset_path: ../data/popularity_bad/popularity_bad.tsv
    train_path: ../data/popularity_bad_big1/popularity_bad_for_bp_ur_fold_1_train.tsv
    test_path: ../data/popularity_bad_big1/popularity_bad_for_bp_ur_fold_1_test.tsv
  
  top_k: 10 # length of recommended list
  evaluation:
    # cutoffs: [10] # eg nDCG@5 and @10
    # relevance_threshold: 0 # minimum value of rating to consider a test transcation relevant for the evaluation process, 0 is the default
    # paired_ttest: True # whether there will be significance tests
    # wilcoxon_test: True
    simple_metrics: [nDCG] # skip RMSE for now because I think it results in weird results
  # gpu: -1
  models:
    DMF:
      meta:
        save_recs: True
      user_mlp: (64,32)
      item_mlp: (64,32)
      epochs: 10 #ok
      batch_size: 256 #ok
      lr: 0.0001 #ok
      reg: 0.001 #ok 
      similarity: cosine
      n_fold: 1