{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddba4adc-8a00-4868-a413-62c14a9eb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module description:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "__version__ = '0.3.1'\n",
    "__author__ = 'Vito Walter Anelli, Claudio Pomo, Daniele Malitesta'\n",
    "__email__ = 'vitowalter.anelli@poliba.it, claudio.pomo@poliba.it, daniele.malitesta@poliba.it'\n",
    "\n",
    "import random\n",
    "from ast import literal_eval as make_tuple\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from elliot.dataset.samplers import custom_sampler as cs\n",
    "from elliot.recommender import BaseRecommenderModel\n",
    "from elliot.recommender.base_recommender_model import init_charger\n",
    "from elliot.recommender.graph_based.ngcf.NGCF_model import NGCFModel\n",
    "from elliot.recommender.recommender_utils_mixin import RecMixin\n",
    "from elliot.utils.write import store_recommendation\n",
    "\n",
    "\n",
    "\n",
    "class NGCF(RecMixin, BaseRecommenderModel):\n",
    "    r\"\"\"\n",
    "    Neural Graph Collaborative Filtering\n",
    "\n",
    "    For further details, please refer to the `paper <https://dl.acm.org/doi/10.1145/3331184.3331267>`_\n",
    "\n",
    "    Args:\n",
    "        lr: Learning rate\n",
    "        epochs: Number of epochs\n",
    "        factors: Number of latent factors\n",
    "        batch_size: Batch size\n",
    "        l_w: Regularization coefficient\n",
    "        weight_size: Tuple with number of units for each embedding propagation layer\n",
    "        node_dropout: Tuple with dropout rate for each node\n",
    "        message_dropout: Tuple with dropout rate for each embedding propagation layer\n",
    "        n_fold: Number of folds to split the adjacency matrix into sub-matrices and ease the computation\n",
    "\n",
    "    To include the recommendation model, add it to the config file adopting the following pattern:\n",
    "\n",
    "    .. code:: yaml\n",
    "\n",
    "      models:\n",
    "        NGCF:\n",
    "          meta:\n",
    "            save_recs: True\n",
    "          lr: 0.0005\n",
    "          epochs: 50\n",
    "          batch_size: 512\n",
    "          factors: 64\n",
    "          batch_size: 256\n",
    "          l_w: 0.1\n",
    "          weight_size: (64,)\n",
    "          node_dropout: ()\n",
    "          message_dropout: (0.1,)\n",
    "          n_fold: 5\n",
    "    \"\"\"\n",
    "    @init_charger\n",
    "    def __init__(self, data, config, params, *args, **kwargs):\n",
    "\n",
    "        self._ratings = self._data.train_dict\n",
    "        self._sampler = cs.Sampler(self._data.i_train_dict)\n",
    "        if self._batch_size < 1:\n",
    "            self._batch_size = self._num_users\n",
    "\n",
    "        ######################################\n",
    "\n",
    "        self._params_list = [\n",
    "            (\"_learning_rate\", \"lr\", \"lr\", 0.0005, None, None),\n",
    "            (\"_factors\", \"latent_dim\", \"factors\", 64, None, None),\n",
    "            (\"_l_w\", \"l_w\", \"l_w\", 0.01, None, None),\n",
    "            (\"_weight_size\", \"weight_size\", \"weight_size\", \"(64,)\", lambda x: list(make_tuple(x)),\n",
    "             lambda x: self._batch_remove(str(x), \" []\").replace(\",\", \"-\")),\n",
    "            (\"_node_dropout\", \"node_dropout\", \"node_dropout\", \"()\", lambda x: list(make_tuple(x)),\n",
    "             lambda x: self._batch_remove(str(x), \" []\").replace(\",\", \"-\")),\n",
    "            (\"_message_dropout\", \"message_dropout\", \"message_dropout\", \"(0.1,)\", lambda x: list(make_tuple(x)),\n",
    "             lambda x: self._batch_remove(str(x), \" []\").replace(\",\", \"-\")),\n",
    "            (\"_n_fold\", \"n_fold\", \"n_fold\", 5, None, None)\n",
    "        ]\n",
    "        self.autoset_params()\n",
    "\n",
    "        self._n_layers = len(self._weight_size)\n",
    "\n",
    "        self._adjacency, self._laplacian = self._create_adj_mat()\n",
    "\n",
    "        self._model = NGCFModel(\n",
    "            num_users=self._num_users,\n",
    "            num_items=self._num_items,\n",
    "            learning_rate=self._learning_rate,\n",
    "            embed_k=self._factors,\n",
    "            l_w=self._l_w,\n",
    "            weight_size=self._weight_size,\n",
    "            n_layers=self._n_layers,\n",
    "            node_dropout=self._node_dropout,\n",
    "            message_dropout=self._message_dropout,\n",
    "            n_fold=self._n_fold,\n",
    "            adjacency=self._adjacency,\n",
    "            laplacian=self._laplacian,\n",
    "            random_seed=self._seed\n",
    "        )\n",
    "\n",
    "    def _create_adj_mat(self):\n",
    "        adjacency = sp.dok_matrix((self._num_users + self._num_items,\n",
    "                                   self._num_users + self._num_items), dtype=np.float32)\n",
    "        adjacency = adjacency.tolil()\n",
    "        ratings = self._data.sp_i_train.tolil()\n",
    "\n",
    "        adjacency[:self._num_users, self._num_users:] = ratings\n",
    "        adjacency[self._num_users:, :self._num_users] = ratings.T\n",
    "        adjacency = adjacency.todok()\n",
    "\n",
    "        def normalized_adj_bi(adj):\n",
    "            # This is exactly how it's done in the paper. Different normalization approaches might be followed.\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "            rowsum += 1e-7  # to avoid division by zero warnings\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "            bi_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "            return bi_adj.tocoo()\n",
    "\n",
    "        laplacian = normalized_adj_bi(adjacency)\n",
    "\n",
    "        return adjacency.tocsr(), laplacian.tocsr()\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"NGCF\" \\\n",
    "               + f\"_{self.get_base_params_shortcut()}\" \\\n",
    "               + f\"_{self.get_params_shortcut()}\"\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        if self._restore:\n",
    "            return self.restore_weights()\n",
    "\n",
    "        for it in self.iterate(self._epochs):\n",
    "            loss = 0\n",
    "            steps = 0\n",
    "            with tqdm(total=int(self._data.transactions // self._batch_size), disable=not self._verbose) as t:\n",
    "                for batch in self._sampler.step(self._data.transactions, self._batch_size):\n",
    "                    steps += 1\n",
    "                    loss += self._model.train_step(batch)\n",
    "                    t.set_postfix({'loss': f'{loss.numpy() / steps:.5f}'})\n",
    "                    t.update()\n",
    "\n",
    "            self.evaluate(it, loss.numpy()/(it + 1))\n",
    "\n",
    "\n",
    "\n",
    "    def get_recommendations(self, k: int = 100):\n",
    "        predictions_top_k_test = {}\n",
    "        predictions_top_k_val = {}\n",
    "        for index, offset in enumerate(range(0, self._num_users, self._batch_size)):\n",
    "            offset_stop = min(offset + self._batch_size, self._num_users)\n",
    "            predictions = self._model.predict(offset, offset_stop)\n",
    "            recs_val, recs_test = self.process_protocol(k, predictions, offset, offset_stop)\n",
    "            predictions_top_k_val.update(recs_val)\n",
    "            predictions_top_k_test.update(recs_test)\n",
    "        return predictions_top_k_val, predictions_top_k_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cbaa4f2-fe02-44a7-b33f-eca29ce9efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"fairbook\"\n",
    "# user-item interactions\n",
    "fairbook_ratings = pd.read_csv(\"elliot/data/fairbook/\"+data+\"_events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "188c9109-daad-4dde-9ae2-37979d49ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(fairbook_ratings.user.unique())\n",
    "num_items = len(fairbook_ratings.item.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32419eb0-e4f4-4836-b082-27d107ab2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = sp.dok_matrix((num_users + num_items,\n",
    "                               num_users + num_items), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "745580d4-33d7-478f-bc14-8fabfeaaa9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = adjacency.tolil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66862fab-ed92-4c11-ba22-fe1521ed54b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13279x13279 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 0 stored elements in List of Lists format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0798672-7f5a-4822-a326-ca67f5966ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_adj_mat():\n",
    "    adjacency = sp.dok_matrix((num_users + num_items,\n",
    "                               num_users + num_items), dtype=np.float32)\n",
    "    \n",
    "    ratings = self._data.sp_i_train.tolil()\n",
    "\n",
    "    adjacency[:self._num_users, self._num_users:] = ratings\n",
    "    adjacency[self._num_users:, :self._num_users] = ratings.T\n",
    "    adjacency = adjacency.todok()\n",
    "\n",
    "    def normalized_adj_bi(adj):\n",
    "        # This is exactly how it's done in the paper. Different normalization approaches might be followed.\n",
    "        rowsum = np.array(adj.sum(1))\n",
    "        rowsum += 1e-7  # to avoid division by zero warnings\n",
    "\n",
    "        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "        bi_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "        return bi_adj.tocoo()\n",
    "\n",
    "    laplacian = normalized_adj_bi(adjacency)\n",
    "\n",
    "    return adjacency.tocsr(), laplacian.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a47bbef-b027-4667-95b7-4eae16acef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elliot.evaluation.metrics.accuracy.ndcg.ndcg import nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38f98d56-0d2e-4187-92af-88316676ba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class nDCG in module elliot.evaluation.metrics.accuracy.ndcg.ndcg:\n",
      "\n",
      "class nDCG(elliot.evaluation.metrics.base_metric.BaseMetric)\n",
      " |  nDCG(recommendations, config, params, eval_objects)\n",
      " |  \n",
      " |  normalized Discounted Cumulative Gain\n",
      " |  \n",
      " |  This class represents the implementation of the nDCG recommendation metric.\n",
      " |  \n",
      " |  For further details, please refer to the `link <https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG>`_\n",
      " |  \n",
      " |  .. math::\n",
      " |      \\begin{gather}\n",
      " |          \\mathrm {DCG@K}=\\sum_{i=1}^{K} \\frac{2^{rel_i}-1}{\\log_{2}{(i+1)}}\\\\\n",
      " |          \\mathrm {IDCG@K}=\\sum_{i=1}^{K}\\frac{1}{\\log_{2}{(i+1)}}\\\\\n",
      " |          \\mathrm {NDCG_u@K}=\\frac{DCG_u@K}{IDCG_u@K}\\\\\n",
      " |          \\mathrm {NDCG@K}=\\frac{\\sum \\nolimits_{u \\in u^{te}NDCG_u@K}}{|u^{te}|}\n",
      " |      \\end{gather}\n",
      " |  \n",
      " |  \n",
      " |  :math:`K` stands for recommending :math:`K` items.\n",
      " |  \n",
      " |  And the :math:`rel_i` is the relevance of the item in position :math:`i` in the recommendation list.\n",
      " |  \n",
      " |  :math:`2^{rel_i}` equals to 1 if the item hits otherwise 0.\n",
      " |  \n",
      " |  :math:`U^{te}` is for all users in the test set.\n",
      " |  \n",
      " |  To compute the metric, add it to the config file adopting the following pattern:\n",
      " |  \n",
      " |  .. code:: yaml\n",
      " |  \n",
      " |      simple_metrics: [nDCG]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      nDCG\n",
      " |      elliot.evaluation.metrics.base_metric.BaseMetric\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, recommendations, config, params, eval_objects)\n",
      " |      Constructor\n",
      " |      :param recommendations: list of recommendations in the form {user: [(item1,value1),...]}\n",
      " |      :param config: SimpleNameSpace that represents the configuration of the experiment\n",
      " |      :param params: Parameters of the model\n",
      " |      :param eval_objects: list of objects that may be useful for the computation of the different metrics\n",
      " |  \n",
      " |  compute_idcg(self, user, cutoff: int) -> float\n",
      " |      Method to compute Ideal Discounted Cumulative Gain\n",
      " |      :param gain_map:\n",
      " |      :param cutoff:\n",
      " |      :return:\n",
      " |  \n",
      " |  compute_user_ndcg(self, user_recommendations: List, user, cutoff: int) -> float\n",
      " |      Method to compute normalized Discounted Cumulative Gain\n",
      " |      :param sorted_item_predictions:\n",
      " |      :param gain_map:\n",
      " |      :param cutoff:\n",
      " |      :return:\n",
      " |  \n",
      " |  eval_user_metric(self)\n",
      " |      Evaluation function\n",
      " |      :return: the overall averaged value of normalized Discounted Cumulative Gain per user\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  name()\n",
      " |      Metric Name Getter\n",
      " |      :return: returns the public name of the metric\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from elliot.evaluation.metrics.base_metric.BaseMetric:\n",
      " |  \n",
      " |  eval(self)\n",
      " |  \n",
      " |  get(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from elliot.evaluation.metrics.base_metric.BaseMetric:\n",
      " |  \n",
      " |  needs_full_recommendations()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from elliot.evaluation.metrics.base_metric.BaseMetric:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997fd09-f1b0-4bae-a480-c2866d1b9456",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Maybe try to run the experiment myself to make sure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1c25b-eafc-4bf1-a442-e7883d4ea83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
